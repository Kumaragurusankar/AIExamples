class AppState(State):
    query: str
    structured: dict
    results: list
    final_results: list

initial_state = AppState(
    query=user_query,
    structured={},
    results=[],
    final_results=[]
)

# 6. LangGraph Nodes with safe state updates
def interpret_prompt_node(state: AppState) -> AppState:
    query = state.query
    structured = llm_chain.run(query)
    try:
        structured_dict = json.loads(structured)
    except:
        structured_dict = {}
    return state.copy(update={"structured": structured_dict})

def search_faiss_node(state: AppState) -> AppState:
    structured_dict = state.structured
    query_text = json.dumps(structured_dict)
    query_embedding = get_embedding(query_text)
    D, I = index.search(np.array([query_embedding]).astype('float32'), k=5)
    matches = [flat_texts[i] for i in I[0]]
    return state.copy(update={"results": matches})

def return_results_node(state: AppState) -> AppState:
    return state.copy(update={"final_results": state.results})



--------------


from typing import TypedDict

class AppState(TypedDict):
    query: str
    structured: dict
    results: list
    final_results: list

def interpret_prompt_node(state: AppState) -> AppState:
    query = state["query"]
    structured = llm_chain.run(query)
    try:
        structured_dict = json.loads(structured)
    except:
        structured_dict = {}
    return {**state, "structured": structured_dict}

def search_faiss_node(state: AppState) -> AppState:
    structured_dict = state["structured"]
    query_text = json.dumps(structured_dict)
    query_embedding = get_embedding(query_text)
    D, I = index.search(np.array([query_embedding]).astype('float32'), k=5)
    matches = [flat_texts[i] for i in I[0]]
    return {**state, "results": matches}

def return_results_node(state: AppState) -> AppState:
    return {**state, "final_results": state["results"]}


initial_state: AppState = {
    "query": user_query,
    "structured": {},
    "results": [],
    "final_results": []
}

------------------------------------------------
def filter_results_node(state):
    query = state["query"]
    candidates = state["results"]
    response = refine_chain.run({
        "query": query,
        "candidates": json.dumps(candidates, indent=2)
    })
    return {"final_output": response}


graph.add_node("FilterResults", filter_results_node)
graph.add_edge("ReturnResults", "FilterResults")
graph.set_finish_point("FilterResults")

result["final_output"]

------------------------

refine_prompt = PromptTemplate(
    input_variables=["query", "candidates"],
    template="""
You are a reasoning assistant. A user asked: "{query}".

Here are 5 candidate XML records (in JSON):
{candidates}

Based on the query, which of these is most relevant?
Respond with the best matching JSON or a ranked list.
"""
)

-----------------------------


result = runnable_graph.invoke({"query": user_query, "structured": {}, "results": [], "final_output": ""})

        st.subheader("ðŸ”Ž Final LLM-Filtered Output")
        if "final_output" in result:
            try:
                final_json = json.loads(result["final_output"])
                st.json(final_json)
            except json.JSONDecodeError:
                st.markdown(result["final_output"])
        else:
            st.warning("No final output returned by the LLM.")
